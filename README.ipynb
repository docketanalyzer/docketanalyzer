{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# docketanalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install with pip. You can then run `da configure` to store API keys and other configuration options.\n",
    "\n",
    "```\n",
    "pip install docketanalyzer\n",
    "da configure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "A minimal, highly-hackable framework for creating interactive agentic workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart: Create a Weather Agent in 30 lines\n",
    "\n",
    "<details>\n",
    "<summary>Show example</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define some tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import Field\n",
    "from docketanalyzer import Agent, BaseTool, notabs\n",
    "\n",
    "class Unit(str, Enum):\n",
    "    f = 'f'\n",
    "    c = 'c'\n",
    "\n",
    "class get_weather(BaseTool):\n",
    "    \"\"\"\n",
    "    Determine the weather in a specified location.\n",
    "    \"\"\"\n",
    "    location: str = Field(..., description=\"The location to get the weather for.\")\n",
    "    unit: Unit = Field(..., description=\"The unit of temperature: 'f' for Fahrenheit or 'c' for Celsius.\")\n",
    "\n",
    "    def __call__(self):\n",
    "        tool_output = user_output = (\n",
    "            f\"## Tool Output\\n\\n```\\nThe weather in {self.location} is currently 90 degrees {self.unit}.\\n```\"\n",
    "            f\"\\n\\nWe have exchanged {len(self.agent.messages)} messages.\"\n",
    "        )\n",
    "        return tool_output, user_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create an agent with tools and instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherAgent(Agent):\n",
    "    name = \"weather-agent\"\n",
    "    tools = [get_weather]\n",
    "    instructions = notabs(\"\"\"\n",
    "    Chat with the user about whatever. If they ask about the weather, use your neat weather tool!\n",
    "    You are *really* excited about getting to use your weather tool.\n",
    "    Even when talking about other stuff, subtly nudge the conversation towards the weather.\n",
    "    Be subtle though! Don't let the user catch on. Make them think it was *their* idea to ask about the weather.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build a UI for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "agent = WeatherAgent()\n",
    "\n",
    "def chat_fn(text):\n",
    "    for streaming_message in agent(text):\n",
    "        yield gr.Chatbot(agent.get_gradio_messages())\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(type=\"messages\")\n",
    "    msg_input = gr.Textbox(show_label=False, submit_btn=True)\n",
    "    msg = gr.Textbox(visible=False, interactive=False)\n",
    "\n",
    "    msg_input.submit(\n",
    "        lambda x: (\"\", x), [msg_input], [msg_input, msg],\n",
    "    ).then(chat_fn, [msg], [chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "\n",
    "### Build a Document Summarization Agent with Working Memory\n",
    "\n",
    "<details>\n",
    "<summary>Show Example</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we create an agent with a self-managed working memory that it can update. This is useful for tasks where you want to update or truncate the agent's message history at each step to save on context tokens.\n",
    "\n",
    "#### 1. Add some tools\n",
    "\n",
    "The `WorkingMemoryMixin` comes with three tools for adding/editing/deleting items in working memory. So we will create one additional tool for the model to signal when it's ready to move on to the next page. The `next_page` tool will interrupt and end the chat stream before the model can respond with a message about the tools it just used (which are wasted tokens for a more automated workflow like this one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from docketanalyzer import Agent, BaseTool, WorkingMemoryMixin, extract_pages, notabs, package_data\n",
    "\n",
    "class next_page(BaseTool):\n",
    "    \"\"\"\n",
    "    Move to the next page of the document. Always call this after your working memory updates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self):\n",
    "        self.agent.done = True\n",
    "        return 'Success', None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create an agent with working memory\n",
    "\n",
    "To use the `WorkingMemoryMixin` effectively, you should update your `agent.messages` to include your working memory state in `agent.working_memory_text`.\n",
    "\n",
    "For agents `__call__` is intended to be the main entrypoint for using an agent. By defauly `__call__` simply wraps `agent.chat`, however this should be overriden for more custom workflows.\n",
    "\n",
    "We also include a few hooks you can override for adding event handlers `on_tool_call` and `on_new_message`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationAgent(Agent, WorkingMemoryMixin):\n",
    "    name = 'summarization-agent'\n",
    "    tools = [next_page]\n",
    "    instructions= notabs(\"\"\"\n",
    "    We are summarizing long documents. We will do this in two stages:\n",
    "    - First you will iterate through the pages of the document building up a list of notes.\n",
    "    - Then there will be a summarization stage where you will use your notes to produce a final output.\n",
    "\n",
    "    We are currently on the first stage. You will only be able to see the current page and your working memory state.\n",
    "    Use your tools to add, edit, and delete notes in your working memory.\n",
    "    These notes should be long, informative, and useful for the summarization stage.\n",
    "    At the same time, you should edit and delete these notes to keep them concise as they do contribute to your context window.\n",
    "    Include references to specific page numbers and quotes from the original text where appropriate.\n",
    "    Call tools as needed until you are ready to proceed to the next page.\n",
    "    Do not respond with any messages to the user. Just use your tools.\n",
    "    \"\"\")\n",
    "\n",
    "    def __init__(self, pages_per_step=5):\n",
    "        super().__init__() # always call super if you override __init__!\n",
    "        self.pages_per_step = pages_per_step\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear() # same for agent.clear\n",
    "        self.pages = []\n",
    "        self.current_page = 0\n",
    "        self.summary = None\n",
    "\n",
    "    @property\n",
    "    def working_memory_tokens(self):\n",
    "        return len(self.chat_model.tokenize(self.working_memory_text))\n",
    "\n",
    "    @property\n",
    "    def state_message(self): # we will use this as the user message\n",
    "        template = notabs(\"\"\"\n",
    "        # Current State\n",
    "\n",
    "        <working_memory>\n",
    "        {working_memory}\n",
    "        </working_memory>\n",
    "\n",
    "        Your working memory is currently using {working_memory_tokens}. \n",
    "        Try to keep this below 8000 tokens by consolidating these notes.\n",
    "\n",
    "        <pages>\n",
    "        {pages}\n",
    "        </pages>\n",
    "\n",
    "        We are currently viewing pages {start} to {end} of {total_pages}.\n",
    "        \"\"\")\n",
    "        pages_text = [x['text'] for x in self.pages[self.current_page:self.current_page + self.pages_per_step]]\n",
    "        pages_text = '\\n\\n---\\n\\n'.join(pages_text)\n",
    "        text = template.replace('{working_memory}', self.working_memory_text)\n",
    "        text = text.replace('{working_memory_tokens}', str(self.working_memory_tokens))\n",
    "        text = text.replace('{pages}', pages_text)\n",
    "        text = text.replace('{start}', str(self.current_page + 1))\n",
    "        text = text.replace('{end}', str(self.current_page + 5))\n",
    "        text = text.replace('{total_pages}', str(len(self.pages)))\n",
    "        return text\n",
    "\n",
    "    def summarize(self): # A one-off chat to get the final summary\n",
    "        print(\"generating final summary\")\n",
    "        notes = '\\n'.join(['- ' + x for x in self.working_memory])\n",
    "        prompt = notabs(f\"\"\"\n",
    "        We are summarizing a long document. We have condensed the document into the following notes:\n",
    "\n",
    "        {notes}\n",
    "\n",
    "        Generate a narrative-driven summary about the document based on these notes.\n",
    "        Go into as much detail as possible, and make sure to reference specific page numbers and quotes from the original text where available.\n",
    "        Aim for a final summary between 2000 and 4000 tokens.\n",
    "        \"\"\")\n",
    "        self.summary = self.chat_model(prompt, **self.chat_args)\n",
    "        return self.summary\n",
    "    \n",
    "    def __call__(self, pages): # Custom entrypoint for processing iteratively self.pages_per_step pages at a time\n",
    "        self.clear()\n",
    "        self.pages = pages\n",
    "        for current_page in tqdm(list(range(0, len(self.pages), self.pages_per_step))):\n",
    "            self.current_page = current_page\n",
    "            self.messages = [] # at each step we reset the message history\n",
    "            for streaming_message in self.chat(self.state_message):\n",
    "                yield streaming_message\n",
    "        self.summarize()\n",
    "    \n",
    "    def on_tool_call(self, tool_name, arguments, tool_output, user_output):\n",
    "        print(\"Page:\", self.current_page, \"Working Memory Tokens:\", self.working_memory_tokens, \"> using tool:\", tool_name, arguments)\n",
    "    \n",
    "    def on_new_message(self):\n",
    "        print(self.messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the agent\n",
    "\n",
    "We will first extract the text from a pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = package_data('example-complaint').path\n",
    "pages = extract_pages(pdf_path)\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the agent's stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SummarizationAgent()\n",
    "for _ in agent(pages):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can view the agent's final summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(len(agent.chat_model.tokenize(agent.summary)), \"tokens\")\n",
    "pprint(agent.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Build Agents with Llama and other Open Source Models with VLLM\n",
    "\n",
    "<details>\n",
    "<summary>Show example</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extend the SummarizationAgent with custom chat_model_args\n",
    "\n",
    "We will extend the `SummarizationAgent` from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLLMSummarizationAgent(SummarizationAgent):\n",
    "    chat_model_args = dict(\n",
    "        mode='vllm', model='meta-llama/Llama-3.2-3B-Instruct',\n",
    "        base_url='YOUR_VLLM_HOST', api_key='YOUR_API_KEY'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Start your vllm service\n",
    "\n",
    "Something like this. The important things are to include `--enable-auto-tool-choice`, `--tool-call-parser`, and `--chat-template`. See the vllm docs for models that support tool use. Currently these include Mistral, Hermes, and newer Llama models.\n",
    "\n",
    "```\n",
    "vllm serve \\\n",
    "    --host 0.0.0.0 --port 8000 \\\n",
    "    --model meta-llama/Llama-3.2-3B-Instruct \\\n",
    "    --dtype bfloat16 --enforce-eager --gpu-memory-utilization 0.95 \\\n",
    "    --api-key YOUR_API_KEY \\\n",
    "    --max-model-len 32000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser llama3_json \\\n",
    "    --chat-template /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pdf_path = package_data('example-complaint').path\n",
    "pages = extract_pages(pdf_path)\n",
    "\n",
    "agent = VLLMSummarizationAgent()\n",
    "\n",
    "for _ in agent(pages):\n",
    "    pass\n",
    "\n",
    "pprint(agent.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document OCR\n",
    "\n",
    "We have a simple utility for extracting text from PDFs. This is mostly taken from Free Law Project's [Doctor](https://github.com/freelawproject/doctor). The main tweak is that `extract_pages` preserves page breaks. \n",
    "\n",
    "You should install `tesseract` on your system to leverage OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docketanalyzer import extract_pages, package_data\n",
    "\n",
    "pdf_path = package_data('example-complaint').path\n",
    "pages = extract_pages(pdf_path)\n",
    "\n",
    "# pages = [\n",
    "#   {'text': 'The first page text ...', 'page': 1, 'ocr_needed': True},\n",
    "#   {'text': 'The second page text ...', 'page': 2, 'ocr_needed': False},\n",
    "#   ...\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docket Manager\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Extensions\n",
    "\n",
    "Use the following to install all the needed requirements for the ML extensions:\n",
    "\n",
    "```\n",
    "pip install 'docketanalyzer[ml]'\n",
    "```\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "TODO\n",
    "\n",
    "### Training Routines\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook README.ipynb to markdown\n",
      "[NbConvertApp] Writing 10937 bytes to README.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown README.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
